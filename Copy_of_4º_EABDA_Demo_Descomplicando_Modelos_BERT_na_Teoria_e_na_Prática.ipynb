{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of 4º EABDA_Demo_Descomplicando Modelos BERT na Teoria e na Prática.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srgari/Jupyter-Notebook-Labs/blob/master/Copy_of_4%C2%BA_EABDA_Demo_Descomplicando_Modelos_BERT_na_Teoria_e_na_Pr%C3%A1tica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Blnr_z4-tR"
      },
      "source": [
        "# **Instalando requerimentos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFH05AbP49bm",
        "outputId": "fcc89c6d-8478-422b-d53f-e2ad3160028d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7792cdd868481cb909091ebd5bcfdc0c6a87de8c7fba55c56ff0f261b3bddc80\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35LVJ3zJ5RKv"
      },
      "source": [
        "# **Baixando dados e scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9hmejiA5Qcr",
        "outputId": "9732f604-2f16-4380-b7af-12fcf30a6f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# Gdrive URL: https://drive.google.com/file/d/1hFktphjfqzAJyeaUJ0dU4DcN4vEFWo8g/view?usp=sharing\n",
        "! gdown --id 1hFktphjfqzAJyeaUJ0dU4DcN4vEFWo8g -O BERT_notebook.zip\n",
        "! unzip BERT_notebook.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hFktphjfqzAJyeaUJ0dU4DcN4vEFWo8g\n",
            "To: /content/BERT_notebook.zip\n",
            "\r0.00B [00:00, ?B/s]\r5.44MB [00:00, 85.3MB/s]\n",
            "Archive:  BERT_notebook.zip\n",
            "   creating: data/\n",
            "  inflating: data/dataset_real.tsv   \n",
            "  inflating: data/dataset_toy.tsv    \n",
            "   creating: scripts/\n",
            "   creating: scripts/.ipynb_checkpoints/\n",
            "  inflating: scripts/.ipynb_checkpoints/BERT_Prаtica-checkpoint.ipynb  \n",
            "  inflating: scripts/BERT_Prаtica.ipynb  \n",
            "   creating: scripts/images/\n",
            "  inflating: scripts/images/model_pieces.png  \n",
            "  inflating: scripts/images/output_pieces.png  \n",
            "  inflating: scripts/images/pooled_output.png  \n",
            "  inflating: scripts/images/tokenizer_outputs.png  \n",
            "  inflating: scripts/images/token_encoder.png  \n",
            "  inflating: scripts/images/word_outputs.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUzffdul5pAj"
      },
      "source": [
        "# Entrar na pasta scripts\n",
        "import os\n",
        "os.chdir('scripts/')\n",
        "\n",
        "# ignore future Warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOA3nq-R5yWf"
      },
      "source": [
        "# **Importações**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdW_lMZm45-o"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FK4pjs78JGb"
      },
      "source": [
        "# **Carregando o modelo e tokenizador a serem utilizados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzqigM7s45-3"
      },
      "source": [
        "#Carregue o modelo e tokenizador a serem usados:\n",
        "pre_trained_model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pre_trained_model_name)\n",
        "model = BertModel.from_pretrained(pre_trained_model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw6uMObs45_F"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/token_encoder.png\", width=400)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHX0SeJ8UGV"
      },
      "source": [
        "# **Criando a classe de armazenamento de dados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThWvq2lL45_f"
      },
      "source": [
        "#Classe de manipulação de dataset:\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    #Construtor da classe:\n",
        "    def __init__(self, sentences, labels, tokenizer, max_length):\n",
        "        #Armazene as entradas que serão passadas ao modelo:\n",
        "        self.sentences = sentences\n",
        "        \n",
        "        #Armazene as labels que serão utilizadas para treino/validação/teste:\n",
        "        self.labels = labels\n",
        "        \n",
        "        #Armazene o tokenizador:\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        #Armazene o tamanho máximo das sentenças:\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    #Retorna o número de instâncias:\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    \n",
        "    #Retorna uma instância completa com base num índice:\n",
        "    def __getitem__(self, index):\n",
        "        #Obtenha a entrada do índice pertinente:\n",
        "        sentence = self.sentences[index]\n",
        "        \n",
        "        #Obtenha a label do índice pertinente:\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        #Tokenize a entrada:\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          sentence,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_length,\n",
        "          return_token_type_ids=True,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "          truncation=True\n",
        "        )\n",
        "    \n",
        "        #Obtenha os códigos numéricos da sentença:\n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        \n",
        "        #Obtenha os códigos numéricos dos token types:\n",
        "        token_type_ids = encoding['token_type_ids'].flatten()\n",
        "        \n",
        "        #Obtenha a máscara de atenção da sentença:\n",
        "        attention_mask = encoding['attention_mask'].flatten()\n",
        "        \n",
        "        #Transforme a label da instância em um tensor:\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        \n",
        "        #Retorne um dicionário com estes dados:\n",
        "        return {\n",
        "          'input_ids': input_ids,\n",
        "          'token_type_ids': token_type_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': label_tensor\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qI4EoMJ45_m"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/tokenizer_outputs.png\", width=600)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsEwR5JY8ez7"
      },
      "source": [
        "# **Criando o carregador de dados**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-U5igF045_x"
      },
      "source": [
        "#Vamos testar o dataset:\n",
        "df = pd.read_csv('../data/dataset_toy.tsv', sep='\\t')\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xBgNQep45_6"
      },
      "source": [
        "#Vamos pegar sentenças e etiquetas do dataset:\n",
        "sentences = df['sentence'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "print(\"Primeira sentença:\", sentences[0])\n",
        "print(\"Primeira etiqueta:\", labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrLiYevj46AC"
      },
      "source": [
        "#Crie um dataset de testes:\n",
        "dtoy = Dataset(sentences, labels, tokenizer, max_length=45)\n",
        "\n",
        "#Pegue uma instância do dataset:\n",
        "data_inst = next(iter(dtoy))\n",
        "\n",
        "#Imprima os componentes da instância:\n",
        "print(\"Input IDs:\", data_inst['input_ids'])\n",
        "print(\"Token Type IDs:\", data_inst['token_type_ids'])\n",
        "print(\"Attention Mask:\", data_inst['attention_mask'])\n",
        "print(\"Labels:\", data_inst['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfG8wwPl46AI"
      },
      "source": [
        "#Crie um DataLoader e coloque o dataset dentro:\n",
        "dltoy = DataLoader(dtoy, batch_size=3)\n",
        "\n",
        "#Pegue uma nova batch do DataLoader:\n",
        "batch = next(iter(dltoy))\n",
        "\n",
        "#Imprima os componentes da batch:\n",
        "print(\"Input IDs:\", batch['input_ids'])\n",
        "print(\"Token Type IDs:\", batch['token_type_ids'])\n",
        "print(\"Attention Mask:\", batch['attention_mask'])\n",
        "print(\"Labels:\", batch['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boHGmeo_8zcK"
      },
      "source": [
        "#**Criando o modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDlZdJhR46AP"
      },
      "source": [
        "#Classificador de sentimento:\n",
        "class SentimentClassifier(torch.nn.Module):\n",
        "\n",
        "    #Construtor da classe\n",
        "    def __init__(self, n_classes, pre_trained_model_name):\n",
        "    \n",
        "        #Inicialize o modelo:\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        \n",
        "        #Carregue um modelo BERT pré-treinado:\n",
        "        self.bert = BertModel.from_pretrained(pre_trained_model_name)\n",
        "        \n",
        "        #Crie a camada linear final para classificação:\n",
        "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    #Função de execução do modelo:\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #Passe a entrada pelo modelo BERT:\n",
        "        word_outputs, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        #Passe a saída \"pooled\" do modelo BERT à camada linear:\n",
        "        return self.linear(pooled_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "pUyUSJ_x46AV"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/model_pieces.png\", width=1000)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bO9PIda46Ad"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/output_pieces.png\", width=1000)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HZjN2aU46Al"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/word_outputs.png\", width=1000)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfI5fVeq46A4"
      },
      "source": [
        "#Vamos visualizar!\n",
        "img = Image(filename = \"images/pooled_output.png\", width=1000)\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5JPcpTw8-xr"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Isto está formatado como código\n",
        "```\n",
        "\n",
        "# **Treinando o modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEdxd3gP46BA"
      },
      "source": [
        "def train_model(model, data_loader, loss_fn, optimizer, scheduler):\n",
        "    #Coloque o modelo em modo de treinamento:\n",
        "    model = model.train()\n",
        "    \n",
        "    #Inicialize o erro total da epoch:\n",
        "    total_loss = 0\n",
        "    total_preds = []\n",
        "    \n",
        "    #Para cada batch do data_loader, faça:\n",
        "    for d in data_loader:\n",
        "        #Obtenha os dados da batch:\n",
        "        input_ids = d[\"input_ids\"]\n",
        "        attention_mask = d[\"attention_mask\"]\n",
        "        labels = d[\"labels\"]\n",
        "        \n",
        "        #Passe os dados pelo modelo:\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        #Obtenha as predições:\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        total_preds.extend([p.item() for p in preds])\n",
        "        \n",
        "        #Calcule o erro:\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        #Propague o erro para o modelo, promovendo aprendizado:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    return total_preds, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZburfUTp46BG"
      },
      "source": [
        "def test_model(model, data_loader, loss_fn):\n",
        "    #Coloque o modelo em modo de treinamento:\n",
        "    model = model.eval()\n",
        "    \n",
        "    #Inicialize o erro total da epoch:\n",
        "    total_loss = 0\n",
        "    total_preds = []\n",
        "    \n",
        "    #Para cada batch do data_loader, faça:\n",
        "    for d in data_loader:\n",
        "        #Obtenha os dados da batch:\n",
        "        input_ids = d[\"input_ids\"]\n",
        "        attention_mask = d[\"attention_mask\"]\n",
        "        labels = d[\"labels\"]\n",
        "        \n",
        "        #Passe os dados pelo modelo:\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        #Obtenha as predições:\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        total_preds.extend([p.item() for p in preds])\n",
        "        \n",
        "        #Calcule o erro:\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "    return total_preds, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAdd6Ust46BL"
      },
      "source": [
        "#Carregue os dados:\n",
        "df = pd.read_csv('../data/dataset_real.tsv', sep='\\t')\n",
        "sentences = df['sentence'].values[:100]\n",
        "labels = df['label'].values[:100]\n",
        "\n",
        "#Defina as proporções de treinamento/validação/teste:\n",
        "tr_prop = 0.9\n",
        "va_prop = 0.05\n",
        "te_prop = 0.05\n",
        "\n",
        "#Obtenha o tamanho do Dataset:\n",
        "dataset_size = len(sentences)\n",
        "\n",
        "#Calcules os índices de divisão de treinamento/validação/teste:\n",
        "tr_limit = int(tr_prop*dataset_size)\n",
        "va_limit = int((tr_prop+va_prop)*dataset_size)\n",
        "te_limit = int((tr_prop+va_prop+te_prop)*dataset_size)\n",
        "\n",
        "#Divida-os para treinamento/validação/teste:\n",
        "sentences_tr = sentences[:tr_limit]\n",
        "labels_tr = labels[:tr_limit]\n",
        "sentences_va = sentences[tr_limit:va_limit]\n",
        "labels_va = labels[tr_limit:va_limit]\n",
        "sentences_te = sentences[va_limit:te_limit]\n",
        "labels_te = labels[va_limit:te_limit]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJoDiKt146BQ"
      },
      "source": [
        "#Estabeleça parâmetros para o Dataset:\n",
        "max_length = 50\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "#Crie Datasets para os dados:\n",
        "dtr = Dataset(sentences_tr, labels_tr, tokenizer, max_length)\n",
        "dva = Dataset(sentences_va, labels_va, tokenizer, max_length)\n",
        "dte = Dataset(sentences_te, labels_te, tokenizer, max_length)\n",
        "\n",
        "#Crie DataLoaders para os Datasets:\n",
        "dltr = DataLoader(dtr, batch_size=batch_size, num_workers=num_workers)\n",
        "dlva = DataLoader(dva, batch_size=batch_size, num_workers=num_workers)\n",
        "dlte = DataLoader(dte, batch_size=batch_size, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZtDpBHJ46BV"
      },
      "source": [
        "#Crie o modelo:\n",
        "sentiment_classifier = SentimentClassifier(n_classes=2, pre_trained_model_name=pre_trained_model_name)\n",
        "\n",
        "#Crie elementos de treinamento:\n",
        "epochs = 3\n",
        "total_steps = len(dltr)*epochs\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(sentiment_classifier.parameters())\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXn4itj546Ba"
      },
      "source": [
        "#Treine o modelo:\n",
        "for i in range(epochs):\n",
        "    #Treine em dados de treinamento:\n",
        "    print('\\nTreinando o modelo, epoch ', i)\n",
        "    preds_tr, total_loss_tr = train_model(sentiment_classifier, dltr, loss_function, optimizer, scheduler)\n",
        "    \n",
        "    #Valide em dados de validação:\n",
        "    print('Validando o modelo, epoch ', i)\n",
        "    preds_va, total_loss_va = test_model(sentiment_classifier, dlva, loss_function)\n",
        "    \n",
        "    #Imprima os erros de treinamento/validação:\n",
        "    print('Erro de treinamento:', total_loss_tr)\n",
        "    print('Erro de validação:', total_loss_va)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgde88FD9FAU"
      },
      "source": [
        "# **Testando o modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_rk4IYw46Bf"
      },
      "source": [
        "#Teste o modelo:\n",
        "preds_te, total_loss_te = test_model(sentiment_classifier, dlte, loss_function)\n",
        "print('Labels reais: ', labels_te)\n",
        "print('Labels preditas: ', preds_te)\n",
        "\n",
        "#Calcule o erro com as etiquetas reais:\n",
        "f1 = f1_score(labels_te, preds_te, average='micro')\n",
        "print('F1-score (micro):', f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}